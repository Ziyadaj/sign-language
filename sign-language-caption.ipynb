{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d08dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "\n",
    "from jiwer import wer\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.preprocessing import image  \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Embedding, Dense, Dropout, Reshape, Layer, Concatenate, MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e5304e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T13:55:49.199404Z",
     "iopub.status.busy": "2024-02-16T13:55:49.199091Z",
     "iopub.status.idle": "2024-02-16T13:56:05.506054Z",
     "shell.execute_reply": "2024-02-16T13:56:05.505289Z"
    },
    "papermill": {
     "duration": 16.317041,
     "end_time": "2024-02-16T13:56:05.508120",
     "exception": false,
     "start_time": "2024-02-16T13:55:49.191079",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 42720 images belonging to 10 classes.\n",
      "Found 5280 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Image Data Generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_dataset = train_datagen.flow_from_directory(\n",
    "    '/kaggle/input/sign-lang/data/train',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_dataset = test_datagen.flow_from_directory(\n",
    "    '/kaggle/input/sign-lang/data/test',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d1469ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T13:56:05.523053Z",
     "iopub.status.busy": "2024-02-16T13:56:05.522525Z",
     "iopub.status.idle": "2024-02-16T13:56:05.535871Z",
     "shell.execute_reply": "2024-02-16T13:56:05.534831Z"
    },
    "papermill": {
     "duration": 0.022885,
     "end_time": "2024-02-16T13:56:05.537880",
     "exception": false,
     "start_time": "2024-02-16T13:56:05.514995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start اسم الله end\n",
      "start الحمد الله end\n",
      "start جميع الصم العرب  السامع end\n",
      "start السلام عليكم رحمة الله بركة end\n",
      "start اليوم اقدم انتم برنامج اخر end\n",
      "start موضوع دراسة لغة الاشارة العربية end\n",
      "start كلمات اليوم متفرقة في الدين end\n",
      "start ايضا كلمات عادية end\n",
      "start لا شرك الله end\n",
      "start الله اكبر end\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "with open('/kaggle/input/sign-lang/data/groundTruth.txt', 'r', encoding='utf-8') as text_file:\n",
    "    text_data = text_file.read().splitlines()\n",
    "\n",
    "init_token, final_token = 'start', 'end'\n",
    "\n",
    "processed_texts = [f\"{init_token} {line} {final_token}\" for line in text_data]\n",
    "tokenizer.fit_on_texts(processed_texts)\n",
    "\n",
    "if init_token not in tokenizer.word_index:\n",
    "    tokenizer.word_index[init_token] = len(tokenizer.word_index) + 1\n",
    "if final_token not in tokenizer.word_index:\n",
    "    tokenizer.word_index[final_token] = len(tokenizer.word_index) + 1\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "text_sequences = tokenizer.texts_to_sequences(processed_texts)\n",
    "sequence_padding = pad_sequences(text_sequences, maxlen=31)\n",
    "\n",
    "for processed_text in processed_texts:\n",
    "    print(processed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7cc679e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T13:56:05.552324Z",
     "iopub.status.busy": "2024-02-16T13:56:05.552062Z",
     "iopub.status.idle": "2024-02-16T13:56:08.119318Z",
     "shell.execute_reply": "2024-02-16T13:56:08.118548Z"
    },
    "papermill": {
     "duration": 2.577024,
     "end_time": "2024-02-16T13:56:08.121657",
     "exception": false,
     "start_time": "2024-02-16T13:56:05.544633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9406464/9406464 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Feature Extraction\n",
    "mobile = MobileNetV2(include_top=False, weights='imagenet', pooling='avg', input_shape=(224, 224, 3))\n",
    "feature_extractor = Model(inputs=mobile.input, outputs=mobile.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f27dbd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T13:56:08.137361Z",
     "iopub.status.busy": "2024-02-16T13:56:08.137084Z",
     "iopub.status.idle": "2024-02-16T13:56:08.144173Z",
     "shell.execute_reply": "2024-02-16T13:56:08.143422Z"
    },
    "papermill": {
     "duration": 0.016936,
     "end_time": "2024-02-16T13:56:08.146078",
     "exception": false,
     "start_time": "2024-02-16T13:56:08.129142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features_from_frames(directory, batch=32):\n",
    "    image_files = [os.path.join(directory, file) for file in sorted(os.listdir(directory)) if file.endswith('.jpg')]\n",
    "    total_frames = len(image_files)\n",
    "    extracted_features = []\n",
    "\n",
    "    for index in range(0, total_frames, batch):\n",
    "        end = min(index + batch, total_frames)\n",
    "        batch_files = image_files[index:end]\n",
    "        \n",
    "        loaded_images = [image.img_to_array(image.load_img(img_path, target_size=(224, 224))) for img_path in batch_files]\n",
    "        loaded_images = np.array(loaded_images) / 255.0\n",
    "\n",
    "        current_features = feature_extractor.predict(loaded_images, verbose=0)\n",
    "        extracted_features.extend(current_features)\n",
    "\n",
    "    final_features = np.array(extracted_features)\n",
    "#     print(\"Processed directory:\", directory, \"Features shape:\", final_features.shape)\n",
    "    return final_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174d3c39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T13:56:08.161280Z",
     "iopub.status.busy": "2024-02-16T13:56:08.160739Z",
     "iopub.status.idle": "2024-02-16T13:59:16.361857Z",
     "shell.execute_reply": "2024-02-16T13:59:16.360995Z"
    },
    "papermill": {
     "duration": 188.211324,
     "end_time": "2024-02-16T13:59:16.364311",
     "exception": false,
     "start_time": "2024-02-16T13:56:08.152987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_fasttext_word_vectors(vector_file):\n",
    "    word_vectors = {}\n",
    "    with io.open(vector_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            word_key = parts[0]\n",
    "            vector_values = np.array(parts[1:], dtype='float32')\n",
    "            word_vectors[word_key] = vector_values\n",
    "    return word_vectors\n",
    "\n",
    "fasttext_vectors = get_fasttext_word_vectors('/kaggle/input/fasttext-arabic-embeddings/cc.ar.300.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a148dc0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T13:59:16.380321Z",
     "iopub.status.busy": "2024-02-16T13:59:16.379978Z",
     "iopub.status.idle": "2024-02-16T13:59:29.445553Z",
     "shell.execute_reply": "2024-02-16T13:59:29.444183Z"
    },
    "papermill": {
     "duration": 13.076538,
     "end_time": "2024-02-16T13:59:29.448304",
     "exception": false,
     "start_time": "2024-02-16T13:59:16.371766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jiwer\r\n",
      "  Obtaining dependency information for jiwer from https://files.pythonhosted.org/packages/0d/4f/ee537ab20144811dd99321735ff92ef2b3a3230b77ed7454bed4c44d21fc/jiwer-3.0.3-py3-none-any.whl.metadata\r\n",
      "  Downloading jiwer-3.0.3-py3-none-any.whl.metadata (2.6 kB)\r\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (8.1.7)\r\n",
      "Requirement already satisfied: rapidfuzz<4,>=3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (3.5.2)\r\n",
      "Downloading jiwer-3.0.3-py3-none-any.whl (21 kB)\r\n",
      "Installing collected packages: jiwer\r\n",
      "Successfully installed jiwer-3.0.3\r\n"
     ]
    }
   ],
   "source": [
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c89f290",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T13:59:29.467175Z",
     "iopub.status.busy": "2024-02-16T13:59:29.466346Z",
     "iopub.status.idle": "2024-02-16T13:59:29.562598Z",
     "shell.execute_reply": "2024-02-16T13:59:29.561651Z"
    },
    "papermill": {
     "duration": 0.108442,
     "end_time": "2024-02-16T13:59:29.564944",
     "exception": false,
     "start_time": "2024-02-16T13:59:29.456502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_wer(ground_truth, hypothesis):\n",
    "    return wer(ground_truth, hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8b6eaf0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T13:59:29.582499Z",
     "iopub.status.busy": "2024-02-16T13:59:29.582174Z",
     "iopub.status.idle": "2024-02-16T13:59:29.593358Z",
     "shell.execute_reply": "2024-02-16T13:59:29.592522Z"
    },
    "papermill": {
     "duration": 0.022376,
     "end_time": "2024-02-16T13:59:29.595311",
     "exception": false,
     "start_time": "2024-02-16T13:59:29.572935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_features_for_signer(signer_path):\n",
    "    \"\"\"Extract features for a given signer's folder.\"\"\"\n",
    "    features = extract_features_from_frames(signer_path)\n",
    "    if features.shape != (80, 1280):\n",
    "        print(f\"Alert: Wrong feature shape in {os.path.basename(signer_path)}: {features.shape}\")\n",
    "        return None\n",
    "    return features\n",
    "\n",
    "def prepare_sequence_for_sentence(sentence_index, truth_texts, tokenizer, max_len):\n",
    "    \"\"\"Prepare padded sequence for a given sentence.\"\"\"\n",
    "    ground_truth_text = truth_texts[sentence_index - 1]\n",
    "    sequence = tokenizer.texts_to_sequences([ground_truth_text])\n",
    "    return pad_sequences(sequence, maxlen=max_len)[0]\n",
    "\n",
    "def process_folder_and_prepare_data(root_path, tokenizer, truth_texts, max_len=31):\n",
    "    features_collection, sequences_collection = [], []\n",
    "\n",
    "    for sentence_dir in sorted(os.listdir(root_path)):\n",
    "        sentence_path = os.path.join(root_path, sentence_dir)\n",
    "        if os.path.isdir(sentence_path):\n",
    "            signer_dirs = [os.path.join(sentence_path, sd) for sd in sorted(os.listdir(sentence_path)) if os.path.isdir(os.path.join(sentence_path, sd))]\n",
    "            valid_features = [extract_features_for_signer(sd) for sd in signer_dirs]\n",
    "            features_collection.extend([f for f in valid_features if f is not None])\n",
    "\n",
    "            if valid_features:\n",
    "                padded_sequence = prepare_sequence_for_sentence(int(sentence_dir), truth_texts, tokenizer, max_len)\n",
    "                sequences_collection.extend([padded_sequence] * len(signer_dirs))\n",
    "\n",
    "    return np.array(features_collection), np.array(sequences_collection)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0c4300a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T13:59:29.612967Z",
     "iopub.status.busy": "2024-02-16T13:59:29.612279Z",
     "iopub.status.idle": "2024-02-16T13:59:29.617605Z",
     "shell.execute_reply": "2024-02-16T13:59:29.616861Z"
    },
    "papermill": {
     "duration": 0.015528,
     "end_time": "2024-02-16T13:59:29.619473",
     "exception": false,
     "start_time": "2024-02-16T13:59:29.603945",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_dim = 300  \n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = fasttext_vectors.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbd49df2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T13:59:29.635933Z",
     "iopub.status.busy": "2024-02-16T13:59:29.635400Z",
     "iopub.status.idle": "2024-02-16T14:04:52.621510Z",
     "shell.execute_reply": "2024-02-16T14:04:52.620550Z"
    },
    "papermill": {
     "duration": 323.00413,
     "end_time": "2024-02-16T14:04:52.631276",
     "exception": false,
     "start_time": "2024-02-16T13:59:29.627146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of training_features: (534, 80, 1280)\n"
     ]
    }
   ],
   "source": [
    "training_features, training_seq = process_folder_and_prepare_data(\n",
    "    '/kaggle/input/sign-lang/data/train', \n",
    "    tokenizer, \n",
    "    processed_texts\n",
    ")\n",
    "print(\"Dimensions of training_features:\", training_features.shape)\n",
    "\n",
    "training_features = np.flip(training_features, axis=1)\n",
    "\n",
    "training_targets = np.zeros_like(training_seq)\n",
    "training_targets[:, :-1] = training_seq[:, 1:]\n",
    "\n",
    "one_hot_targets = tf.keras.utils.to_categorical(training_targets, num_classes=vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d542f29",
   "metadata": {
    "papermill": {
     "duration": 0.007216,
     "end_time": "2024-02-16T14:04:52.646022",
     "exception": false,
     "start_time": "2024-02-16T14:04:52.638806",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Encoder Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d01b5461",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T14:04:52.662154Z",
     "iopub.status.busy": "2024-02-16T14:04:52.661842Z",
     "iopub.status.idle": "2024-02-16T14:05:07.687492Z",
     "shell.execute_reply": "2024-02-16T14:05:07.686577Z"
    },
    "papermill": {
     "duration": 15.035978,
     "end_time": "2024-02-16T14:05:07.689468",
     "exception": false,
     "start_time": "2024-02-16T14:04:52.653490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "17/17 [==============================] - 5s 19ms/step - loss: 1.1981\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.6783\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.5530\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.4570\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.3692\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.2955\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.2311\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.1694\n",
      "Epoch 9/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.1337\n",
      "Epoch 10/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.1161\n",
      "Epoch 11/30\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.1083\n",
      "Epoch 12/30\n",
      "17/17 [==============================] - 0s 19ms/step - loss: 0.0903\n",
      "Epoch 13/30\n",
      "17/17 [==============================] - 0s 19ms/step - loss: 0.0694\n",
      "Epoch 14/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0571\n",
      "Epoch 15/30\n",
      "17/17 [==============================] - 0s 20ms/step - loss: 0.0506\n",
      "Epoch 16/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0419\n",
      "Epoch 17/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0430\n",
      "Epoch 18/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0480\n",
      "Epoch 19/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0486\n",
      "Epoch 20/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0362\n",
      "Epoch 21/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0231\n",
      "Epoch 22/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0166\n",
      "Epoch 23/30\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.0163\n",
      "Epoch 24/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0125\n",
      "Epoch 25/30\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.0142\n",
      "Epoch 26/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0184\n",
      "Epoch 27/30\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.0183\n",
      "Epoch 28/30\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.0112\n",
      "Epoch 29/30\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.0067\n",
      "Epoch 30/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0058\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7b23d11e0100>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoder-Decoder Model Definition\n",
    "encoder_inputs = Input(shape=(80, 1280))  \n",
    "# encoder_gru = GRU(units=256, return_state=True)\n",
    "encoder_lstm = LSTM(units=256, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "# encoder_outputs, state_h = encoder_gru(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, \n",
    "                              weights=[embedding_matrix], \n",
    "                              trainable=False)\n",
    "\n",
    "decoder_lstm = LSTM(units=256, return_sequences=True, return_state=True)\n",
    "# decoder_gru = GRU(units=256, return_sequences=True, return_state=True)\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "\n",
    "x = decoder_embedding(decoder_inputs)\n",
    "x = Dropout(0.25)(x)\n",
    "# x, _ = decoder_gru(x, initial_state=state_h)\n",
    "x, _, _ = decoder_lstm(x, initial_state=encoder_states)\n",
    "x = Dropout(0.25)(x)\n",
    "decoder_outputs = decoder_dense(x)\n",
    "\n",
    "no_attention = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "no_attention.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Training the model\n",
    "no_attention.fit([training_features, training_seq], one_hot_targets, batch_size=32, epochs=30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5389392b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T14:05:07.742341Z",
     "iopub.status.busy": "2024-02-16T14:05:07.742050Z",
     "iopub.status.idle": "2024-02-16T14:05:53.820208Z",
     "shell.execute_reply": "2024-02-16T14:05:53.819122Z"
    },
    "papermill": {
     "duration": 46.106873,
     "end_time": "2024-02-16T14:05:53.822245",
     "exception": false,
     "start_time": "2024-02-16T14:05:07.715372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 706ms/step\n",
      "Predicted:                           start start لا الله end \n",
      "Actual:  start اسم الله end\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted:                           start start ايضا الله end \n",
      "Actual:  start اسم الله end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                           start start ايضا الله end \n",
      "Actual:  start اسم الله end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                           start start ايضا الله end \n",
      "Actual:  start اسم الله end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                            start اسم الله end \n",
      "Actual:  start اسم الله end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                            start اسم الله end \n",
      "Actual:  start اسم الله end\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted:                          start start start لا الله end \n",
      "Actual:  start الحمد الله end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          start start start لا الله end \n",
      "Actual:  start الحمد الله end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          start start start لا الله end \n",
      "Actual:  start الحمد الله end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                            start اسم الله end \n",
      "Actual:  start الحمد الله end\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted:                            start ايضا الله end \n",
      "Actual:  start الحمد الله end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                            start اسم الله end \n",
      "Actual:  start الحمد الله end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                           كلمات الصم العرب السامع end \n",
      "Actual:  start جميع الصم العرب  السامع end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                         start start كلمات اليوم العرب السامع end \n",
      "Actual:  start جميع الصم العرب  السامع end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          start اليوم اليوم العرب السامع end \n",
      "Actual:  start جميع الصم العرب  السامع end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          start كلمات الصم العرب السامع end \n",
      "Actual:  start جميع الصم العرب  السامع end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                           كلمات الصم العرب السامع end \n",
      "Actual:  start جميع الصم العرب  السامع end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                           كلمات كلمات العرب السامع end \n",
      "Actual:  start جميع الصم العرب  السامع end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                         start موضوع دراسة رحمة الله بركة end \n",
      "Actual:  start السلام عليكم رحمة الله بركة end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                         start موضوع دراسة رحمة الله بركة end \n",
      "Actual:  start السلام عليكم رحمة الله بركة end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                         start السلام عليكم رحمة الله بركة end \n",
      "Actual:  start السلام عليكم رحمة الله بركة end\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted:                         start كلمات عليكم رحمة الله بركة end \n",
      "Actual:  start السلام عليكم رحمة الله بركة end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          كلمات عليكم رحمة الله بركة end \n",
      "Actual:  start السلام عليكم رحمة الله بركة end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          كلمات كلمات رحمة الله بركة end \n",
      "Actual:  start السلام عليكم رحمة الله بركة end\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted:                           عليكم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                           عليكم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted:                            انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          السلام عليكم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          السلام عليكم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted:                          السلام عليكم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          السلام عليكم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted:                           عليكم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted:                         start موضوع دراسة انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          السلام اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                         start موضوع دراسة انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted:                          موضوع اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "Predicted:                          السلام دراسة لغة الاشارة العربية end \n",
      "Actual:  start موضوع دراسة لغة الاشارة العربية end\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted:                         start السلام دراسة لغة الاشارة العربية end \n",
      "Actual:  start موضوع دراسة لغة الاشارة العربية end\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted:                          موضوع دراسة لغة الاشارة العربية end \n",
      "Actual:  start موضوع دراسة لغة الاشارة العربية end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          موضوع دراسة لغة الاشارة العربية end \n",
      "Actual:  start موضوع دراسة لغة الاشارة العربية end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          موضوع دراسة لغة الاشارة العربية end \n",
      "Actual:  start موضوع دراسة لغة الاشارة العربية end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          موضوع دراسة لغة الاشارة العربية end \n",
      "Actual:  start موضوع دراسة لغة الاشارة العربية end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          كلمات اليوم متفرقة في الدين end \n",
      "Actual:  start كلمات اليوم متفرقة في الدين end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          كلمات اليوم متفرقة في الدين end \n",
      "Actual:  start كلمات اليوم متفرقة في الدين end\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted:                          كلمات اليوم متفرقة في الدين end \n",
      "Actual:  start كلمات اليوم متفرقة في الدين end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          كلمات اليوم متفرقة في الدين end \n",
      "Actual:  start كلمات اليوم متفرقة في الدين end\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted:                          كلمات اليوم متفرقة في الدين end \n",
      "Actual:  start كلمات اليوم متفرقة في الدين end\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted:                           اليوم متفرقة في الدين end \n",
      "Actual:  start كلمات اليوم متفرقة في الدين end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                           start ايضا كلمات عادية end \n",
      "Actual:  start ايضا كلمات عادية end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                            ايضا كلمات عادية end \n",
      "Actual:  start ايضا كلمات عادية end\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted:                           start ايضا كلمات عادية end \n",
      "Actual:  start ايضا كلمات عادية end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                           start ايضا كلمات عادية end \n",
      "Actual:  start ايضا كلمات عادية end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                           start ايضا كلمات عادية end \n",
      "Actual:  start ايضا كلمات عادية end\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted:                            ايضا كلمات عادية end \n",
      "Actual:  start ايضا كلمات عادية end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                           start لا شرك الله end \n",
      "Actual:  start لا شرك الله end\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted:                          start start لا شرك الله end \n",
      "Actual:  start لا شرك الله end\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted:                            لا شرك الله end \n",
      "Actual:  start لا شرك الله end\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted:                            لا شرك الله end \n",
      "Actual:  start لا شرك الله end\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted:                            لا شرك الله end \n",
      "Actual:  start لا شرك الله end\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted:                            لا شرك الله end \n",
      "Actual:  start لا شرك الله end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          start start start لا اكبر end \n",
      "Actual:  start الله اكبر end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                           start start ايضا اكبر end \n",
      "Actual:  start الله اكبر end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                           start start لا اكبر end \n",
      "Actual:  start الله اكبر end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                           start start ايضا اكبر end \n",
      "Actual:  start الله اكبر end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                            start ايضا اكبر end \n",
      "Actual:  start الله اكبر end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                           start start ايضا اكبر end \n",
      "Actual:  start الله اكبر end\n",
      "Overall Word Error Rate (WER): 0.28801587301587295\n"
     ]
    }
   ],
   "source": [
    "def interpret_sequence(sequence, tokenizer):\n",
    "    # Interpreting a sequence into a caption, where 'sequence' is a 2D array (time steps, vocabulary size)\n",
    "    caption = []\n",
    "    \n",
    "    for token_probs in sequence:\n",
    "        # Choosing the index with the highest probability\n",
    "        selected_index = np.argmax(token_probs)\n",
    "        interpreted_word = tokenizer.index_word.get(selected_index, '')\n",
    "        caption.append(interpreted_word)\n",
    "\n",
    "    return ' '.join(caption)\n",
    "\n",
    "\n",
    "def process_and_evaluate_folder(folder_path, model, tokenizer, captions_list):\n",
    "    wer_metrics = []\n",
    "\n",
    "    for signer_directory in sorted(os.listdir(folder_path)):\n",
    "        path_to_signer = os.path.join(folder_path, signer_directory)\n",
    "        if os.path.isdir(path_to_signer):\n",
    "            video_features = extract_features_from_frames(path_to_signer)\n",
    "            video_features = np.expand_dims(video_features, axis=0)  # Reshaping for model prediction\n",
    "\n",
    "            # Retrieve the actual caption for the current directory\n",
    "            directory_num = int(os.path.basename(folder_path))\n",
    "            actual_caption = captions_list[directory_num - 1]\n",
    "            \n",
    "            # Preparing the input sequence for the model\n",
    "            input_seq = tokenizer.texts_to_sequences([actual_caption])\n",
    "            model_input_seq = pad_sequences(input_seq, maxlen=31)\n",
    "\n",
    "            # Generating the predicted caption sequence\n",
    "            predicted_sequence = no_attention.predict([video_features, model_input_seq])\n",
    "            generated_caption = interpret_sequence(predicted_sequence[0], tokenizer)\n",
    "            \n",
    "            print('Predicted: ', generated_caption)\n",
    "            print('Actual: ', actual_caption)\n",
    "            \n",
    "            # Calculating Word Error Rate (WER)\n",
    "            wer_score = calculate_wer(actual_caption, generated_caption)\n",
    "            wer_metrics.append(wer_score)\n",
    "\n",
    "    return np.mean(wer_metrics)\n",
    "\n",
    "\n",
    "# Testing the model\n",
    "data_path = '/kaggle/input/sign-lang/data/test'\n",
    "wer_scores_list = []\n",
    "\n",
    "for sentence_directory in sorted(os.listdir(data_path)):\n",
    "    directory_path = os.path.join(data_path, sentence_directory)\n",
    "    if os.path.isdir(directory_path):\n",
    "        average_wer = process_and_evaluate_folder(directory_path, no_attention, tokenizer, processed_texts)\n",
    "        wer_scores_list.append(average_wer)\n",
    "\n",
    "overall_wer = np.mean(wer_scores_list)\n",
    "print(\"Overall Word Error Rate (WER):\", overall_wer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868e34cb",
   "metadata": {
    "papermill": {
     "duration": 0.040673,
     "end_time": "2024-02-16T14:05:53.949257",
     "exception": false,
     "start_time": "2024-02-16T14:05:53.908584",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Encoder Decoder Model with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5841516b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T14:05:54.033629Z",
     "iopub.status.busy": "2024-02-16T14:05:54.032886Z",
     "iopub.status.idle": "2024-02-16T14:05:54.040013Z",
     "shell.execute_reply": "2024-02-16T14:05:54.039117Z"
    },
    "papermill": {
     "duration": 0.051521,
     "end_time": "2024-02-16T14:05:54.042001",
     "exception": false,
     "start_time": "2024-02-16T14:05:53.990480",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W = Dense(units, use_bias=False)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query shape == (batch_size, seq_len, hidden_size)\n",
    "        # values shape == (batch_size, hidden_size, seq_len)\n",
    "        values_transposed = tf.transpose(values, perm=[0, 2, 1])\n",
    "        \n",
    "        # Calculate the attention scores\n",
    "        score = tf.matmul(query, values_transposed)\n",
    "        \n",
    "        # attention_weights shape == (batch_size, seq_len, seq_len)\n",
    "        attention_weights = tf.nn.softmax(score, axis=-1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, seq_len, hidden_size)\n",
    "        context_vector = tf.matmul(attention_weights, values)\n",
    "        \n",
    "        return context_vector, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b7d0b93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T14:05:54.126091Z",
     "iopub.status.busy": "2024-02-16T14:05:54.125325Z",
     "iopub.status.idle": "2024-02-16T14:06:07.679761Z",
     "shell.execute_reply": "2024-02-16T14:06:07.678968Z"
    },
    "papermill": {
     "duration": 13.598629,
     "end_time": "2024-02-16T14:06:07.681653",
     "exception": false,
     "start_time": "2024-02-16T14:05:54.083024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "17/17 [==============================] - 4s 17ms/step - loss: 1.1802\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.5768\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.4499\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.3495\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.2580\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.2006\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.1496\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.1174\n",
      "Epoch 9/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.1023\n",
      "Epoch 10/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.0846\n",
      "Epoch 11/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.0685\n",
      "Epoch 12/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.0823\n",
      "Epoch 13/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0615\n",
      "Epoch 14/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0511\n",
      "Epoch 15/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0605\n",
      "Epoch 16/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.0558\n",
      "Epoch 17/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0412\n",
      "Epoch 18/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0329\n",
      "Epoch 19/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0791\n",
      "Epoch 20/30\n",
      "17/17 [==============================] - 0s 18ms/step - loss: 0.0565\n",
      "Epoch 21/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0418\n",
      "Epoch 22/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0311\n",
      "Epoch 23/30\n",
      "17/17 [==============================] - 0s 17ms/step - loss: 0.0280\n",
      "Epoch 24/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.0226\n",
      "Epoch 25/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.0199\n",
      "Epoch 26/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.0219\n",
      "Epoch 27/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.0238\n",
      "Epoch 28/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.0270\n",
      "Epoch 29/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.0171\n",
      "Epoch 30/30\n",
      "17/17 [==============================] - 0s 16ms/step - loss: 0.0149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7b23cbc84e20>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoder\n",
    "encoder_inputs = Input(shape=(80, 1280))  \n",
    "encoder_gru = GRU(units=256, return_sequences=True, return_state=True)\n",
    "encoder_outputs, state_h = encoder_gru(encoder_inputs)\n",
    "\n",
    "# Attention Layer\n",
    "attention_layer = Attention(256)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "decoder_embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], trainable=False)\n",
    "decoder_gru = GRU(units=256, return_sequences=True)\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "\n",
    "x = decoder_embedding(decoder_inputs)\n",
    "decoder_outputs = decoder_gru(x, initial_state=state_h)\n",
    "\n",
    "# Applying attention\n",
    "attention_layer = Attention(256)\n",
    "context_vector, attention_weights = attention_layer(decoder_outputs, encoder_outputs)\n",
    "\n",
    "# Concatenate context vector with decoder outputs\n",
    "decoder_combined_context = Concatenate(axis=-1)([context_vector, decoder_outputs])\n",
    "\n",
    "# Output layer\n",
    "output = decoder_dense(decoder_combined_context)\n",
    "\n",
    "# Define the model\n",
    "attention = Model([encoder_inputs, decoder_inputs], output)\n",
    "\n",
    "\n",
    "attention.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "attention.fit([training_features, training_seq], one_hot_targets, batch_size=32, epochs=30) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d90f8d68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T14:06:07.799117Z",
     "iopub.status.busy": "2024-02-16T14:06:07.798307Z",
     "iopub.status.idle": "2024-02-16T14:06:38.087916Z",
     "shell.execute_reply": "2024-02-16T14:06:38.086942Z"
    },
    "papermill": {
     "duration": 30.349901,
     "end_time": "2024-02-16T14:06:38.089828",
     "exception": false,
     "start_time": "2024-02-16T14:06:07.739927",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 615ms/step\n",
      "Predicted:                           start start اسم الله end \n",
      "Actual:  start اسم الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                           start start اسم الله end \n",
      "Actual:  start اسم الله end\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted:                           start start اسم الله end \n",
      "Actual:  start اسم الله end\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted:                          start start start ايضا الله end \n",
      "Actual:  start اسم الله end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                           start start ايضا الله end \n",
      "Actual:  start اسم الله end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          start start start ايضا الله end \n",
      "Actual:  start اسم الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          start start start الله الله end \n",
      "Actual:  start الحمد الله end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                         start start start start الله الله end \n",
      "Actual:  start الحمد الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                           start start الله الله end \n",
      "Actual:  start الحمد الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          start start start اسم الله end \n",
      "Actual:  start الحمد الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          start start start اسم الله end \n",
      "Actual:  start الحمد الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          start start start اسم الله end \n",
      "Actual:  start الحمد الله end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          start اليوم الصم العرب السامع end \n",
      "Actual:  start جميع الصم العرب  السامع end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                         start start ايضا الصم العرب السامع end \n",
      "Actual:  start جميع الصم العرب  السامع end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                         start start لا الصم العرب السامع end \n",
      "Actual:  start جميع الصم العرب  السامع end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          start كلمات الصم العرب السامع end \n",
      "Actual:  start جميع الصم العرب  السامع end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          start اليوم الصم العرب السامع end \n",
      "Actual:  start جميع الصم العرب  السامع end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          start كلمات الصم العرب السامع end \n",
      "Actual:  start جميع الصم العرب  السامع end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          كلمات عليكم رحمة الله بركة end \n",
      "Actual:  start السلام عليكم رحمة الله بركة end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          اليوم عليكم رحمة الله بركة end \n",
      "Actual:  start السلام عليكم رحمة الله بركة end\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted:                          كلمات عليكم رحمة الله بركة end \n",
      "Actual:  start السلام عليكم رحمة الله بركة end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          كلمات عليكم رحمة الله بركة end \n",
      "Actual:  start السلام عليكم رحمة الله بركة end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          كلمات عليكم رحمة الله بركة end \n",
      "Actual:  start السلام عليكم رحمة الله بركة end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          كلمات عليكم رحمة الله بركة end \n",
      "Actual:  start السلام عليكم رحمة الله بركة end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          كلمات اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          كلمات اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          موضوع اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          كلمات اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          كلمات اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          كلمات اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted:                          كلمات اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          كلمات اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted:                          السلام اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted:                          السلام اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted:                         start السلام اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          كلمات اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted:                         start كلمات دراسة لغة الاشارة العربية end \n",
      "Actual:  start موضوع دراسة لغة الاشارة العربية end\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted:                         start كلمات دراسة لغة الاشارة العربية end \n",
      "Actual:  start موضوع دراسة لغة الاشارة العربية end\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted:                         start اليوم دراسة لغة الاشارة العربية end \n",
      "Actual:  start موضوع دراسة لغة الاشارة العربية end\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted:                         start اليوم دراسة لغة الاشارة العربية end \n",
      "Actual:  start موضوع دراسة لغة الاشارة العربية end\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted:                         start السلام دراسة لغة الاشارة العربية end \n",
      "Actual:  start موضوع دراسة لغة الاشارة العربية end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                         start السلام دراسة لغة الاشارة العربية end \n",
      "Actual:  start موضوع دراسة لغة الاشارة العربية end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          كلمات اليوم متفرقة في الدين end \n",
      "Actual:  start كلمات اليوم متفرقة في الدين end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          كلمات اليوم متفرقة في الدين end \n",
      "Actual:  start كلمات اليوم متفرقة في الدين end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          كلمات اليوم متفرقة في الدين end \n",
      "Actual:  start كلمات اليوم متفرقة في الدين end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                         start اليوم اليوم متفرقة في الدين end \n",
      "Actual:  start كلمات اليوم متفرقة في الدين end\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted:                         start اليوم اليوم متفرقة في الدين end \n",
      "Actual:  start كلمات اليوم متفرقة في الدين end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                         start اليوم اليوم متفرقة في الدين end \n",
      "Actual:  start كلمات اليوم متفرقة في الدين end\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted:                          start start ايضا كلمات عادية end \n",
      "Actual:  start ايضا كلمات عادية end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          start start ايضا كلمات عادية end \n",
      "Actual:  start ايضا كلمات عادية end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          start start ايضا كلمات عادية end \n",
      "Actual:  start ايضا كلمات عادية end\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted:                          start start ايضا كلمات عادية end \n",
      "Actual:  start ايضا كلمات عادية end\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted:                          start start ايضا كلمات عادية end \n",
      "Actual:  start ايضا كلمات عادية end\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted:                           start ايضا كلمات عادية end \n",
      "Actual:  start ايضا كلمات عادية end\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted:                          start start الله شرك الله end \n",
      "Actual:  start لا شرك الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          start start الله شرك الله end \n",
      "Actual:  start لا شرك الله end\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted:                           start ايضا شرك الله end \n",
      "Actual:  start لا شرك الله end\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted:                           start ايضا شرك الله end \n",
      "Actual:  start لا شرك الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                           start ايضا شرك الله end \n",
      "Actual:  start لا شرك الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          start start ايضا شرك الله end \n",
      "Actual:  start لا شرك الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                           start start اسم اكبر end \n",
      "Actual:  start الله اكبر end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                           start start الله اكبر end \n",
      "Actual:  start الله اكبر end\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted:                           start start الله اكبر end \n",
      "Actual:  start الله اكبر end\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted:                           start start ايضا اكبر end \n",
      "Actual:  start الله اكبر end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          start start start ايضا اكبر end \n",
      "Actual:  start الله اكبر end\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted:                          start start start ايضا اكبر end \n",
      "Actual:  start الله اكبر end\n",
      "Overall Word Error Rate (WER): 0.3242460317460317\n"
     ]
    }
   ],
   "source": [
    "def interpret_sequence(sequence, tokenizer):\n",
    "    # Interpreting a sequence into a caption, where 'sequence' is a 2D array (time steps, vocabulary size)\n",
    "    caption = []\n",
    "    \n",
    "    for token_probs in sequence:\n",
    "        # Choosing the index with the highest probability\n",
    "        selected_index = np.argmax(token_probs)\n",
    "        interpreted_word = tokenizer.index_word.get(selected_index, '')\n",
    "        caption.append(interpreted_word)\n",
    "\n",
    "    return ' '.join(caption)\n",
    "\n",
    "\n",
    "def process_and_evaluate_folder(folder_path, model, tokenizer, captions_list):\n",
    "    wer_metrics = []\n",
    "\n",
    "    for signer_directory in sorted(os.listdir(folder_path)):\n",
    "        path_to_signer = os.path.join(folder_path, signer_directory)\n",
    "        if os.path.isdir(path_to_signer):\n",
    "            video_features = extract_features_from_frames(path_to_signer)\n",
    "            video_features = np.expand_dims(video_features, axis=0)  # Reshaping for model prediction\n",
    "\n",
    "            # Retrieve the actual caption for the current directory\n",
    "            directory_num = int(os.path.basename(folder_path))\n",
    "            actual_caption = captions_list[directory_num - 1]\n",
    "            \n",
    "            # Preparing the input sequence for the model\n",
    "            input_seq = tokenizer.texts_to_sequences([actual_caption])\n",
    "            model_input_seq = pad_sequences(input_seq, maxlen=31)\n",
    "\n",
    "            # Generating the predicted caption sequence\n",
    "            predicted_sequence = attention.predict([video_features, model_input_seq])\n",
    "            generated_caption = interpret_sequence(predicted_sequence[0], tokenizer)\n",
    "            \n",
    "            print('Predicted: ', generated_caption)\n",
    "            print('Actual: ', actual_caption)\n",
    "            # Calculating Word Error Rate (WER)\n",
    "            wer_score = calculate_wer(actual_caption, generated_caption)\n",
    "            wer_metrics.append(wer_score)\n",
    "\n",
    "    return np.mean(wer_metrics)\n",
    "\n",
    "\n",
    "# Testing the model\n",
    "data_path = '/kaggle/input/sign-lang/data/test'\n",
    "wer_scores_list = []\n",
    "\n",
    "for sentence_directory in sorted(os.listdir(data_path)):\n",
    "    directory_path = os.path.join(data_path, sentence_directory)\n",
    "    if os.path.isdir(directory_path):\n",
    "        average_wer = process_and_evaluate_folder(directory_path, attention, tokenizer, processed_texts)\n",
    "        wer_scores_list.append(average_wer)\n",
    "\n",
    "overall_wer = np.mean(wer_scores_list)\n",
    "print(\"Overall Word Error Rate (WER):\", overall_wer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9669ec",
   "metadata": {
    "papermill": {
     "duration": 0.07142,
     "end_time": "2024-02-16T14:06:38.234741",
     "exception": false,
     "start_time": "2024-02-16T14:06:38.163321",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86ce6b2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T14:06:38.381673Z",
     "iopub.status.busy": "2024-02-16T14:06:38.381300Z",
     "iopub.status.idle": "2024-02-16T14:06:38.819843Z",
     "shell.execute_reply": "2024-02-16T14:06:38.818877Z"
    },
    "papermill": {
     "duration": 0.533667,
     "end_time": "2024-02-16T14:06:38.839927",
     "exception": false,
     "start_time": "2024-02-16T14:06:38.306260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_6 (InputLayer)        [(None, 80, 1280)]           0         []                            \n",
      "                                                                                                  \n",
      " layer_normalization (Layer  (None, 80, 1280)             2560      ['input_6[0][0]']             \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " multi_head_attention (Mult  (None, 80, 1280)             2624256   ['layer_normalization[0][0]', \n",
      " iHeadAttention)                                                     'layer_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " input_7 (InputLayer)        [(None, None)]               0         []                            \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 80, 1280)             0         ['multi_head_attention[0][0]']\n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)     (None, None, 300)            9900      ['input_7[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOp  (None, 80, 1280)             0         ['dropout_2[0][0]',           \n",
      " Lambda)                                                             'input_6[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_2 (Lay  (None, None, 300)            600       ['embedding_2[0][0]']         \n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " layer_normalization_1 (Lay  (None, 80, 1280)             2560      ['tf.__operators__.add[0][0]']\n",
      " erNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " multi_head_attention_1 (Mu  (None, None, 300)            616236    ['layer_normalization_2[0][0]'\n",
      " ltiHeadAttention)                                                  , 'layer_normalization_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 80, 512)              655872    ['layer_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)         (None, None, 300)            0         ['multi_head_attention_1[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 80, 1280)             656640    ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TF  (None, None, 300)            0         ['dropout_4[0][0]',           \n",
      " OpLambda)                                                           'embedding_2[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)         (None, 80, 1280)             0         ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " layer_normalization_3 (Lay  (None, None, 300)            600       ['tf.__operators__.add_2[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TF  (None, 80, 1280)             0         ['dropout_3[0][0]',           \n",
      " OpLambda)                                                           'tf.__operators__.add[0][0]']\n",
      "                                                                                                  \n",
      " multi_head_attention_2 (Mu  (None, None, 300)            1619756   ['layer_normalization_3[0][0]'\n",
      " ltiHeadAttention)                                                  , 'tf.__operators__.add_1[0][0\n",
      "                                                                    ]']                           \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)         (None, None, 300)            0         ['multi_head_attention_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " tf.__operators__.add_3 (TF  (None, None, 300)            0         ['dropout_5[0][0]',           \n",
      " OpLambda)                                                           'tf.__operators__.add_2[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " layer_normalization_4 (Lay  (None, None, 300)            600       ['tf.__operators__.add_3[0][0]\n",
      " erNormalization)                                                   ']                            \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, None, 512)            154112    ['layer_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, None, 300)            153900    ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)         (None, None, 300)            0         ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      " tf.__operators__.add_4 (TF  (None, None, 300)            0         ['dropout_6[0][0]',           \n",
      " OpLambda)                                                           'tf.__operators__.add_3[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, None, 33)             9933      ['tf.__operators__.add_4[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6507525 (24.82 MB)\n",
      "Trainable params: 6507525 (24.82 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = Dense(inputs.shape[-1])(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    return x + res\n",
    "\n",
    "def transformer_decoder(inputs, enc_outputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Encoder-Decoder Attention\n",
    "    x = LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout)(x, enc_outputs)\n",
    "    x = Dropout(dropout)(x)\n",
    "    res = x + res\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = Dense(ff_dim, activation=\"relu\")(x)\n",
    "    x = Dense(inputs.shape[-1])(x)\n",
    "    x = Dropout(dropout)(x)\n",
    "    return x + res\n",
    "\n",
    "# Hyperparameters\n",
    "feature_dim = 1280  \n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "head_size = 64  \n",
    "ff_dim = 512 \n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(80, feature_dim))  # Adjusted to match your data shape\n",
    "enc_out = transformer_encoder(encoder_inputs, head_size, num_heads, ff_dim, dropout_rate)\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(decoder_inputs)\n",
    "dec_out = transformer_decoder(dec_emb, enc_out, head_size, num_heads, ff_dim, dropout_rate)\n",
    "\n",
    "# Output layer\n",
    "outputs = Dense(vocab_size, activation='softmax')(dec_out)\n",
    "\n",
    "# Build the model\n",
    "transformer = Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs)\n",
    "transformer.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "\n",
    "# Model summary\n",
    "transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd0bf995",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T14:06:38.998997Z",
     "iopub.status.busy": "2024-02-16T14:06:38.998156Z",
     "iopub.status.idle": "2024-02-16T14:07:05.756826Z",
     "shell.execute_reply": "2024-02-16T14:07:05.755912Z"
    },
    "papermill": {
     "duration": 26.840046,
     "end_time": "2024-02-16T14:07:05.758801",
     "exception": false,
     "start_time": "2024-02-16T14:06:38.918755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "17/17 [==============================] - 10s 66ms/step - loss: 3.1993\n",
      "Epoch 2/30\n",
      "17/17 [==============================] - 1s 42ms/step - loss: 1.6208\n",
      "Epoch 3/30\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.7888\n",
      "Epoch 4/30\n",
      "17/17 [==============================] - 1s 42ms/step - loss: 0.6571\n",
      "Epoch 5/30\n",
      "17/17 [==============================] - 1s 42ms/step - loss: 0.2790\n",
      "Epoch 6/30\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1908\n",
      "Epoch 7/30\n",
      "17/17 [==============================] - 1s 32ms/step - loss: 0.1659\n",
      "Epoch 8/30\n",
      "17/17 [==============================] - 1s 43ms/step - loss: 0.1549\n",
      "Epoch 9/30\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1497\n",
      "Epoch 10/30\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1458\n",
      "Epoch 11/30\n",
      "17/17 [==============================] - 1s 32ms/step - loss: 0.1461\n",
      "Epoch 12/30\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1427\n",
      "Epoch 13/30\n",
      "17/17 [==============================] - 1s 42ms/step - loss: 0.1430\n",
      "Epoch 14/30\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1423\n",
      "Epoch 15/30\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1414\n",
      "Epoch 16/30\n",
      "17/17 [==============================] - 1s 42ms/step - loss: 0.1402\n",
      "Epoch 17/30\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1401\n",
      "Epoch 18/30\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1404\n",
      "Epoch 19/30\n",
      "17/17 [==============================] - 1s 30ms/step - loss: 0.1399\n",
      "Epoch 20/30\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1386\n",
      "Epoch 21/30\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1398\n",
      "Epoch 22/30\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1392\n",
      "Epoch 23/30\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1383\n",
      "Epoch 24/30\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1385\n",
      "Epoch 25/30\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1386\n",
      "Epoch 26/30\n",
      "17/17 [==============================] - 1s 42ms/step - loss: 0.1390\n",
      "Epoch 27/30\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1386\n",
      "Epoch 28/30\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1383\n",
      "Epoch 29/30\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1387\n",
      "Epoch 30/30\n",
      "17/17 [==============================] - 1s 31ms/step - loss: 0.1389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7b23cb8d3c70>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.fit([training_features, training_seq], one_hot_targets, batch_size=32, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b145da0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-16T14:07:05.969756Z",
     "iopub.status.busy": "2024-02-16T14:07:05.969091Z",
     "iopub.status.idle": "2024-02-16T14:07:36.500181Z",
     "shell.execute_reply": "2024-02-16T14:07:36.499276Z"
    },
    "papermill": {
     "duration": 30.638101,
     "end_time": "2024-02-16T14:07:36.502039",
     "exception": false,
     "start_time": "2024-02-16T14:07:05.863938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 331ms/step\n",
      "Predicted:                             اسم الله end \n",
      "Actual:  start اسم الله end\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "Predicted:                             اسم الله end \n",
      "Actual:  start اسم الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                             اسم الله end \n",
      "Actual:  start اسم الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                             اسم الله end \n",
      "Actual:  start اسم الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                             اسم الله end \n",
      "Actual:  start اسم الله end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                             اسم الله end \n",
      "Actual:  start اسم الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                             الحمد الله end \n",
      "Actual:  start الحمد الله end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                             الحمد الله end \n",
      "Actual:  start الحمد الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                             الحمد الله end \n",
      "Actual:  start الحمد الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                             الحمد الله end \n",
      "Actual:  start الحمد الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                             الحمد الله end \n",
      "Actual:  start الحمد الله end\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted:                             الحمد الله end \n",
      "Actual:  start الحمد الله end\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted:                           جميع الصم العرب السامع end \n",
      "Actual:  start جميع الصم العرب  السامع end\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted:                           جميع الصم العرب السامع end \n",
      "Actual:  start جميع الصم العرب  السامع end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                           جميع الصم العرب السامع end \n",
      "Actual:  start جميع الصم العرب  السامع end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                           جميع الصم العرب السامع end \n",
      "Actual:  start جميع الصم العرب  السامع end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                           جميع الصم العرب السامع end \n",
      "Actual:  start جميع الصم العرب  السامع end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                           جميع الصم العرب السامع end \n",
      "Actual:  start جميع الصم العرب  السامع end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          السلام عليكم رحمة الله بركة end \n",
      "Actual:  start السلام عليكم رحمة الله بركة end\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted:                          السلام عليكم رحمة الله بركة end \n",
      "Actual:  start السلام عليكم رحمة الله بركة end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          السلام عليكم رحمة الله بركة end \n",
      "Actual:  start السلام عليكم رحمة الله بركة end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          السلام عليكم رحمة الله بركة end \n",
      "Actual:  start السلام عليكم رحمة الله بركة end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          السلام عليكم رحمة الله بركة end \n",
      "Actual:  start السلام عليكم رحمة الله بركة end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          السلام عليكم رحمة الله بركة end \n",
      "Actual:  start السلام عليكم رحمة الله بركة end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          اليوم اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          اليوم اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          اليوم اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          اليوم اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted:                          اليوم اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          اليوم اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          اليوم اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          اليوم اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          اليوم اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          اليوم اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          اليوم اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          اليوم اقدم انتم برنامج اخر end \n",
      "Actual:  start اليوم اقدم انتم برنامج اخر end\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted:                          موضوع دراسة لغة الاشارة العربية end \n",
      "Actual:  start موضوع دراسة لغة الاشارة العربية end\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted:                          موضوع دراسة لغة الاشارة العربية end \n",
      "Actual:  start موضوع دراسة لغة الاشارة العربية end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          موضوع دراسة لغة الاشارة العربية end \n",
      "Actual:  start موضوع دراسة لغة الاشارة العربية end\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "Predicted:                          موضوع دراسة لغة الاشارة العربية end \n",
      "Actual:  start موضوع دراسة لغة الاشارة العربية end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          موضوع دراسة لغة الاشارة العربية end \n",
      "Actual:  start موضوع دراسة لغة الاشارة العربية end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          موضوع دراسة لغة الاشارة العربية end \n",
      "Actual:  start موضوع دراسة لغة الاشارة العربية end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          كلمات اليوم متفرقة في الدين end \n",
      "Actual:  start كلمات اليوم متفرقة في الدين end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          كلمات اليوم متفرقة في الدين end \n",
      "Actual:  start كلمات اليوم متفرقة في الدين end\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Predicted:                          كلمات اليوم متفرقة في الدين end \n",
      "Actual:  start كلمات اليوم متفرقة في الدين end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                          كلمات اليوم متفرقة في الدين end \n",
      "Actual:  start كلمات اليوم متفرقة في الدين end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          كلمات اليوم متفرقة في الدين end \n",
      "Actual:  start كلمات اليوم متفرقة في الدين end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                          كلمات اليوم متفرقة في الدين end \n",
      "Actual:  start كلمات اليوم متفرقة في الدين end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                            ايضا كلمات عادية end \n",
      "Actual:  start ايضا كلمات عادية end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                            ايضا كلمات عادية end \n",
      "Actual:  start ايضا كلمات عادية end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                            ايضا كلمات عادية end \n",
      "Actual:  start ايضا كلمات عادية end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                            ايضا كلمات عادية end \n",
      "Actual:  start ايضا كلمات عادية end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                            ايضا كلمات عادية end \n",
      "Actual:  start ايضا كلمات عادية end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                            ايضا كلمات عادية end \n",
      "Actual:  start ايضا كلمات عادية end\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Predicted:                            لا شرك الله end \n",
      "Actual:  start لا شرك الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                            لا شرك الله end \n",
      "Actual:  start لا شرك الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                            لا شرك الله end \n",
      "Actual:  start لا شرك الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                            لا شرك الله end \n",
      "Actual:  start لا شرك الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                            لا شرك الله end \n",
      "Actual:  start لا شرك الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                            لا شرك الله end \n",
      "Actual:  start لا شرك الله end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                             الله اكبر end \n",
      "Actual:  start الله اكبر end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                             الله اكبر end \n",
      "Actual:  start الله اكبر end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                             الله اكبر end \n",
      "Actual:  start الله اكبر end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                             الله اكبر end \n",
      "Actual:  start الله اكبر end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                             الله اكبر end \n",
      "Actual:  start الله اكبر end\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted:                             الله اكبر end \n",
      "Actual:  start الله اكبر end\n",
      "Overall Word Error Rate (WER): 0.1888095238095238\n"
     ]
    }
   ],
   "source": [
    "def interpret_sequence(sequence, tokenizer):\n",
    "    # Interpreting a sequence into a caption, where 'sequence' is a 2D array (time steps, vocabulary size)\n",
    "    caption = []\n",
    "    \n",
    "    for token_probs in sequence:\n",
    "        # Choosing the index with the highest probability\n",
    "        selected_index = np.argmax(token_probs)\n",
    "        interpreted_word = tokenizer.index_word.get(selected_index, '')\n",
    "        caption.append(interpreted_word)\n",
    "\n",
    "    return ' '.join(caption)\n",
    "\n",
    "\n",
    "def process_and_evaluate_folder(folder_path, model, tokenizer, captions_list):\n",
    "    wer_metrics = []\n",
    "\n",
    "    for signer_directory in sorted(os.listdir(folder_path)):\n",
    "        path_to_signer = os.path.join(folder_path, signer_directory)\n",
    "        if os.path.isdir(path_to_signer):\n",
    "            video_features = extract_features_from_frames(path_to_signer)\n",
    "            video_features = np.expand_dims(video_features, axis=0)  # Reshaping for model prediction\n",
    "\n",
    "            # Retrieve the actual caption for the current directory\n",
    "            directory_num = int(os.path.basename(folder_path))\n",
    "            actual_caption = captions_list[directory_num - 1]\n",
    "            \n",
    "            # Preparing the input sequence for the model\n",
    "            input_seq = tokenizer.texts_to_sequences([actual_caption])\n",
    "            model_input_seq = pad_sequences(input_seq, maxlen=31)\n",
    "\n",
    "            # Generating the predicted caption sequence\n",
    "            predicted_sequence = transformer.predict([video_features, model_input_seq])\n",
    "            generated_caption = interpret_sequence(predicted_sequence[0], tokenizer)\n",
    "            \n",
    "            print('Predicted: ', generated_caption)\n",
    "            print('Actual: ', actual_caption)\n",
    "            # Calculating Word Error Rate (WER)\n",
    "            wer_score = calculate_wer(actual_caption, generated_caption)\n",
    "            wer_metrics.append(wer_score)\n",
    "\n",
    "    return np.mean(wer_metrics)\n",
    "\n",
    "\n",
    "# Testing the model\n",
    "data_path = '/kaggle/input/sign-lang/data/test'\n",
    "wer_scores_list = []\n",
    "\n",
    "for sentence_directory in sorted(os.listdir(data_path)):\n",
    "    directory_path = os.path.join(data_path, sentence_directory)\n",
    "    if os.path.isdir(directory_path):\n",
    "        average_wer = process_and_evaluate_folder(directory_path, transformer, tokenizer, processed_texts)\n",
    "        wer_scores_list.append(average_wer)\n",
    "\n",
    "overall_wer = np.mean(wer_scores_list)\n",
    "print(\"Overall Word Error Rate (WER):\", overall_wer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbe9cbe",
   "metadata": {
    "papermill": {
     "duration": 0.118611,
     "end_time": "2024-02-16T14:07:36.741188",
     "exception": false,
     "start_time": "2024-02-16T14:07:36.622577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4068016,
     "sourceId": 7065167,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4095985,
     "sourceId": 7104924,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4109374,
     "sourceId": 7123904,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30616,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 714.7816,
   "end_time": "2024-02-16T14:07:40.676712",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-02-16T13:55:45.895112",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
